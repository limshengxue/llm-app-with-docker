
services:
  web:
    build:
      context: ./api
      dockerfile: Dockerfile
    ports:
      - 8000:8000
    volumes:
      - ./api:/home
    networks:
      - chatbot-net
    environment:
      - OLLAMA_SERVER_URL=http://ollama:11434
      - CHROMA_SERVER_URL=http://vectordb:8000
      - MODEL_NAME=gemma3:1b
    depends_on:
      ollama:
        condition: service_healthy
      vectordb:
        condition: service_started
      

  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    ports:
      - 11434:11434
    volumes:
      - chatbot-vol:/root/.ollama
    networks:
      - chatbot-net
    environment:
      - MODEL_NAME=gemma3:1b
    entrypoint: [ "/usr/bin/bash", "pull-model.sh" ]
    healthcheck:
      test: 
        - "CMD-SHELL"
        - |
          test -f /tmp/ollama_ready && \
          bash -c '</dev/tcp/localhost/11434'  # Checks if Ollama is accepting connections      interval: 30s
      timeout: 5s
      retries: 10
  
  vectordb:
    image: chromadb/chroma
    ports:
      - 8001:8000
    volumes:
      - chroma-data:/data
    networks:
      - chatbot-net

networks:
  chatbot-net:
    driver: bridge

volumes:
  chatbot-vol:
    driver: local
  chroma-data:
    driver: local